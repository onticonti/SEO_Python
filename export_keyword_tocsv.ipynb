{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "export_keyword_tocsv",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onticonti/SEO_Python/blob/master/export_keyword_tocsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWZ1DrX8H1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from requests.exceptions import Timeout\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        " \n",
        "keyword = input(\"Arama yapÄ±lacak anahtar kelimeyi giriniz : \")\n",
        "filename = keyword.replace(' ', '_')\n",
        " \n",
        "serp = {\n",
        "    'urls': [],\n",
        "    'titles': [],\n",
        "    'meta_desc': [],\n",
        "    'h1': [],\n",
        "    'h2': [],\n",
        "    'h3': [],\n",
        "    'b': [],\n",
        "    'strong': [],\n",
        "    'em': [],\n",
        "    'i': [],\n",
        "    'a': []\n",
        "}\n",
        " \n",
        "params = {\n",
        "    'access_key': 'a7c264f20edb7061f30d3fa0e8ca7503',\n",
        "    'query': keyword\n",
        "}\n",
        " \n",
        "api_result = requests.get('http://api.serpstack.com/search', params, verify=False)\n",
        " \n",
        "api_response = api_result.json()\n",
        " \n",
        "org = api_response.get(\"organic_results\")\n",
        " \n",
        "for results in range(0,len(org)):\n",
        "    link = (org[results]['url'])\n",
        "    serp['urls'].append(link)\n",
        "    serp['titles'].append(org[results]['title'])\n",
        "    serp['meta_desc'].append(org[results]['snippet'])\n",
        "    try:\n",
        "        page = requests.get(link, timeout=5)\n",
        "    except Timeout:\n",
        "        print(\"Timed out!\")\n",
        "    else:\n",
        "        if page.status_code >= 400:\n",
        "            continue\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
        "    for headings in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"b\", \"strong\", \"em\", \"i\", \"a\"]):\n",
        "        if headings.name == 'h1' and headings.text not in serp.keys():\n",
        "            serp['h1'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'h2' and headings.text not in serp.keys():\n",
        "            serp['h2'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'h3' and headings.text not in serp.keys():\n",
        "            serp['h3'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'b' and headings.text not in serp.keys():\n",
        "            serp['b'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'strong' and headings.text not in serp.keys():\n",
        "            serp['strong'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'em' and headings.text not in serp.keys():\n",
        "            serp['em'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'i' and headings.text not in serp.keys():\n",
        "            serp['i'].append(headings.text.replace('\\n',\"\"))\n",
        "        elif headings.name == 'a' and headings not in serp.keys():\n",
        "            serp['a'].append(headings.get('href'))          \n",
        "        \n",
        "df = {key:pd.Series(value) for key, value in serp.items()}\n",
        " \n",
        "serpdata = pd.DataFrame(df)\n",
        "print(serpdata)\n",
        " \n",
        "serpdata.to_csv(f'{filename}.csv', header=True, index=False,encoding=\"utf8\")\n",
        "files.download(f'{filename}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}